---
layout: distill
title: Sentence Segmentation and Why It Matters
description: Accompanying the paper &quot;<a href='https://example.com'>Where’s the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation</a>&quot; with Jonas Pfeiffer and Ivan Vulić, accepted at ACL 2023.
giscus_comments: false
date: 2021-05-22

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
  - name: Self-Supervised Sentence Segmentation
  - name: Where's the Point?
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---

{% include blog/sentence/prelude.html %}
{% include blog/sentence/main.html %}

<span id="introduction" style="padding-top: 4.8rem;">Sentences are fundamental to written language.</span> Naturally, Natural Language Processing often involves sentences. For example, the original BERT did Next *Sentence* Prediction, creation of the C4 corpus involved removing "any page with fewer than 5 *sentences*" and datasets such as MultiNLI consist of annotated *sentence* pairs. In fact, around 72% of the papers in the ACL Anthology contain the word "sentence" (~16% in the abstract). However, many of these say do not say what they mean by "sentence". As usual, BERT is a standout by containing the following admission:

> Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence.

Why do the authors concede all linguistic meaning of a sentence? And what do others mean when they say "sentence"? Like all things involving natural language, it's complicated. Attempts to define sentences go back at least 130 years to Henry Sweet's now-copied-all-over-textbooks definition<d-cite key="sweet2014new"></d-cite>:

> A sentence is a word or group of words capable of expressing a complete thought or meaning.

This definition is not really workable. It moves the problem to another term — what is a complete thought? There are chunks of text which most people would probably agree make up a sentence, but hardly convey a complete thought.<d-footnote>For example "Look, having nuclear — my uncle was a great professor and scientist and engineer, Dr. John Trump at MIT; good genes, very good genes, OK, very smart, the Wharton School of Finance, very good, very smart — you know, if you’re a conservative Republican, if I were a liberal, if, like, OK, if I ran as a liberal Democrat, they would say I'm one of the smartest people anywhere in the world — it’s true! — but when you're a conservative Republican they try — oh, do they do a number — that’s why I always start off: Went to Wharton, was a good student, went there, went there, did this, built a fortune — you know I have to give my like credentials all the time, because we’re a little disadvantaged — but you look at the nuclear deal, the thing that really bothers me — it would have been so easy, and it’s not as important as these lives are — nuclear is so powerful; my uncle explained that to me many, many years ago, the power and that was 35 years ago; he would explain the power of what's going to happen and he was right, who would have thought? — but when you look at what's going on with the four prisoners — now it used to be three, now it’s four — but when it was three and even now, I would have said it's all in the messenger; fellas, and it is fellas because, you know, they don't, they haven’t figured that the women are smarter right now than the men, so, you know, it’s gonna take them about another 150 years — but the Persians are great negotiators, the Iranians are great negotiators, so, and they, they just killed, they just killed us, this is horrible." via <a href="https://www.snopes.com/fact-check/donald-trump-sentence">https://www.snopes.com/fact-check/donald-trump-sentence.</a></d-footnote> As such, when someone refers to "sentences" in NLP, they usually mean one of two things:

1. A chunk of text that resulted from instructing annotators to create a sentence, or to segment an existing text into sentences ("ground-truth sentences").
2. Predicted sentences from an automatic sentence segmentation tool, such as Punkt. Many of these are trained on ground-truth sentences.

The MultiNLI corpus is a collection of the first type. The filtering in the C4 corpus is an example of the second: it wouldn't be feasible to manually segment a large portion of the web into sentences. 

At first glance, it seems like we're good. If it's feasible, use ground-truth sentences created by human annotators. If not, approximate the ground-truth with an automatic sentence segmentation tool. But there are some problems.

__"Ground-truth" sentence boundaries are not objectively true.__

Many things involving language are subjective. It might seem like something as fundamental as sentence segmentation could  at least be solved objectively, but alas! There is plenty of room for interpretation in what to consider one sentence. Does nesting via quotation marks constitute a sentence boundary? What about parentheses, enumerations, colons, and semicolons? Different collections of sentences will have different styles of segmentation. <d-footnote>Style can also vary within a collection via different annotators or inconsistencies of an individual annotator.</d-footnote> For example, sentences in the UD, OPUS100 and Ersatz collections look noticeably different from each other.

{% include blog/sentence/styles.html %}

__It's hard to acquire ground-truth sentences for low-resource languages.__

Like any kind of labelled data, it's hard to get ground-truth sentences for the long tail of low-resource languages. This makes it hard to create sentence segmentation tools for these languages.

__Existing sentence segmentation tools rely on punctuation.__

Existing sentence segmentation tools like Ersatz and Punkt rely on punctuation by treating sentence segmentation as a *disambiguation* task. They look at all punctuation characters in the text, and for each one, decide whether it constitutes a sentence boundary or not. This is a reasonable approach, but it falls apart in some cases. If a text is missing punctuation, it can not be segmented. Also, text in languages which do not use punctuation (most prominently Thai) can not be segmented.

> English: Many sentence segmentation tools rely on punctuation. That can be a problem in Thai.
>
> Thai: เครื่องมือแบ่งส่วนประโยคจำนวนมากใช้เครื่องหมายวรรคตอน นั่นอาจเป็นปัญหาในภาษาไทย
>

We tried to solve these issues by creating a *punctuation-agnostic*, *adaptible* sentence segmentation tool which can be trained without any ground-truth sentences via self-supervision.

## Self-Supervised Sentence Segmentation

Our key insight is that a model which is good at predicting the probability for a __new line (\n)__ to occur after any character can segment text into sentences. This could be a decoder-style model à la GPT trained on next-character prediction. We use an encoder-style model trained with a newline-corruption method instead to use context from both sides of every character. Like pretraining GPT models, this does not require any labelled data; it's just language modelling. 

The model learns how likely it is for any character to be followed by a new line. The predicted *newline probability* characterizes sentence boundaries: a new line can never occur within a sentence, while at the same time, a new line can generally occur after any sentence. This allows us to define sentences in a much more concrete way.

> Tired: A sentence is a word or group of words capable of expressing a complete thought or meaning.
>
> Wired: <strong style="color: var(--global-theme-color);">A sentence is any sequence of characters which could plausibly be followed by a newline.</strong>

Admittedly, the word "plausibly" still does a lot of heavy lifting here: we need a probability threshold to decide when to count a character as "plausibly followed by a newline". However, this threshold can just be set to a small constant value like 1%, where different thresholds give rise to different sets of sentences. 

{% include blog/sentence/probabilities.html %}

<h2 id="where-s-the-point">Where's the Point?</h2>
